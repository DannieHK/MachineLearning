{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNW440lsPe8GN+SxyegM8K5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DannieHK/MachineLearning/blob/main/GPT2textPredicter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK3qDqzQC2Qd",
        "outputId": "f25d8b91-0859-4129-dc54-4f291f4e8d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-11 12:12:57--  https://www.gutenberg.org/cache/epub/73328/pg73328.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290554 (284K) [text/plain]\n",
            "Saving to: ‘THE_OUTERMOST_HOUSE.txt’\n",
            "\n",
            "THE_OUTERMOST_HOUSE 100%[===================>] 283.74K  1.22MB/s    in 0.2s    \n",
            "\n",
            "2024-04-11 12:12:58 (1.22 MB/s) - ‘THE_OUTERMOST_HOUSE.txt’ saved [290554/290554]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O \"sh.txt\" \"https://www.gutenberg.org/cache/epub/73328/pg73328.txt\" -O THE_OUTERMOST_HOUSE.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"THE_OUTERMOST_HOUSE.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Simple preprocessing: split the text into chunks if needed"
      ],
      "metadata": {
        "id": "ecoL0JEiC23I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary libraries are installed and updated\n",
        "!pip install accelerate -U\n",
        "!pip install transformers datasets -U"
      ],
      "metadata": {
        "id": "nnHMMX59C2_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "\n",
        "text_file = 'THE_OUTERMOST_HOUSE.txt'\n",
        "\n",
        "# Create a dataset and a data collator for language modeling\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=text_file,\n",
        "    block_size=128\n",
        ") # Adjust based on your model and preferences\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # For causal (not masked) language modeling\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Where to store the training outputs\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,              # Number of epochs (adjust as needed)\n",
        "    per_device_train_batch_size=4,   # Batch size (adjust based on your GPU)\n",
        "    save_steps=10_000,               # Save the model every 10,000 steps\n",
        "    save_total_limit=2,              # Only keep the 2 most recent checkpoints\n",
        ")\n",
        "\n",
        "# Initialize Trainer.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Saving the trained model.\n",
        "model.save_pretrained('./your_trained_model')\n",
        "tokenizer.save_pretrained('./your_trained_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "4ZOGntbvC3Cn",
        "outputId": "8b9a4f76-e1a1-4418-da78-16e60c05c3a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [720/720 02:15, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.397800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./your_trained_model/tokenizer_config.json',\n",
              " './your_trained_model/special_tokens_map.json',\n",
              " './your_trained_model/vocab.json',\n",
              " './your_trained_model/merges.txt',\n",
              " './your_trained_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)"
      ],
      "metadata": {
        "id": "yZLG8XvSC3Fm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_path = './your_trained_model'  # Update this to your model's directory\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "xOLz6ajkC3IR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt that will make it generate text with same structure as the book downloaded.\n",
        "prompt = \"East and ahead of the\""
      ],
      "metadata": {
        "id": "sPLHJlkUC3K5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')"
      ],
      "metadata": {
        "id": "s8rVBNb5C3NN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "output_sequences = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=100,  # Maximum length of the generated text\n",
        "    num_return_sequences=1,  # Number of generated sequences\n",
        "    temperature=0.7,  # Sampling temperature\n",
        "    top_k=50,  # Only consider the top k tokens at each step\n",
        "    top_p=0.9,  # Nucleus sampling\n",
        "    repetition_penalty=1.2,  # Penalty for repetition\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHSojE1nC3Qm",
        "outputId": "722d2f71-d5a8-45de-8b6d-50f41eb9fcfb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "East and ahead of the\n",
            "sea, I saw a great mass approaching from Cape Cod. The beach was full with surf; there were no fish in it at all! There had been an enormous wave that swept over this whole region on its way to\n",
            "\n",
            "“the west coast last night? ” said Mr Abbott as he stood watching me watch--a huge one indeed!--and then paused for effectual seconds before returning again into his chair after ten oclock or so till eleven: _The\n"
          ]
        }
      ]
    }
  ]
}