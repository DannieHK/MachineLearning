{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNW440lsPe8GN+SxyegM8K5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DannieHK/MachineLearning/blob/main/GPT2textPredicter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK3qDqzQC2Qd",
        "outputId": "f25d8b91-0859-4129-dc54-4f291f4e8d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-11 12:12:57--  https://www.gutenberg.org/cache/epub/73328/pg73328.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290554 (284K) [text/plain]\n",
            "Saving to: ‚ÄòTHE_OUTERMOST_HOUSE.txt‚Äô\n",
            "\n",
            "THE_OUTERMOST_HOUSE 100%[===================>] 283.74K  1.22MB/s    in 0.2s    \n",
            "\n",
            "2024-04-11 12:12:58 (1.22 MB/s) - ‚ÄòTHE_OUTERMOST_HOUSE.txt‚Äô saved [290554/290554]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O \"sh.txt\" \"https://www.gutenberg.org/cache/epub/73328/pg73328.txt\" -O THE_OUTERMOST_HOUSE.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"THE_OUTERMOST_HOUSE.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Simple preprocessing: split the text into chunks if needed"
      ],
      "metadata": {
        "id": "ecoL0JEiC23I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary libraries are installed and updated\n",
        "!pip install accelerate -U\n",
        "!pip install transformers datasets -U"
      ],
      "metadata": {
        "id": "nnHMMX59C2_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "\n",
        "text_file = 'THE_OUTERMOST_HOUSE.txt'\n",
        "\n",
        "# Create a dataset and a data collator for language modeling\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=text_file,\n",
        "    block_size=128\n",
        ") # Adjust based on your model and preferences\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # For causal (not masked) language modeling\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Where to store the training outputs\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,              # Number of epochs (adjust as needed)\n",
        "    per_device_train_batch_size=4,   # Batch size (adjust based on your GPU)\n",
        "    save_steps=10_000,               # Save the model every 10,000 steps\n",
        "    save_total_limit=2,              # Only keep the 2 most recent checkpoints\n",
        ")\n",
        "\n",
        "# Initialize Trainer.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Saving the trained model.\n",
        "model.save_pretrained('./your_trained_model')\n",
        "tokenizer.save_pretrained('./your_trained_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "4ZOGntbvC3Cn",
        "outputId": "8b9a4f76-e1a1-4418-da78-16e60c05c3a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [720/720 02:15, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.397800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./your_trained_model/tokenizer_config.json',\n",
              " './your_trained_model/special_tokens_map.json',\n",
              " './your_trained_model/vocab.json',\n",
              " './your_trained_model/merges.txt',\n",
              " './your_trained_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)"
      ],
      "metadata": {
        "id": "yZLG8XvSC3Fm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_path = './your_trained_model'  # Update this to your model's directory\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "xOLz6ajkC3IR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt that will make it generate text with same structure as the book downloaded.\n",
        "prompt = \"East and ahead of the\""
      ],
      "metadata": {
        "id": "sPLHJlkUC3K5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')"
      ],
      "metadata": {
        "id": "s8rVBNb5C3NN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "output_sequences = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=100,  # Maximum length of the generated text\n",
        "    num_return_sequences=1,  # Number of generated sequences\n",
        "    temperature=0.7,  # Sampling temperature\n",
        "    top_k=50,  # Only consider the top k tokens at each step\n",
        "    top_p=0.9,  # Nucleus sampling\n",
        "    repetition_penalty=1.2,  # Penalty for repetition\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHSojE1nC3Qm",
        "outputId": "722d2f71-d5a8-45de-8b6d-50f41eb9fcfb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "East and ahead of the\n",
            "sea, I saw a great mass approaching from Cape Cod. The beach was full with surf; there were no fish in it at all! There had been an enormous wave that swept over this whole region on its way to\n",
            "\n",
            "‚Äúthe west coast last night? ‚Äù said Mr Abbott as he stood watching me watch--a huge one indeed!--and then paused for effectual seconds before returning again into his chair after ten oclock or so till eleven: _The\n"
          ]
        }
      ]
    }
  ]
}